{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 챗봇 전처리 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 챗봇 전처리 클래스 \n",
    "from konlpy.tag import Komoran\n",
    "import pickle\n",
    "\n",
    "class Preprocess:\n",
    "    def __init__(self, word2index_dic ='', userdic=None):   \n",
    "        # 단어사전 불러오기     \n",
    "        if(word2index_dic !=''):\n",
    "            f = open(word2index_dic, \"rb\")\n",
    "            self.word_index = pickle.load(f)\n",
    "            f.close()\n",
    "        else:\n",
    "            self.word_index=None\n",
    "            \n",
    "        self.komoran = Komoran(userdic=userdic)\n",
    "        self.exclusion_tags = [\n",
    "            'JKS', 'JKC', 'JKG', 'JKO', 'JKB', 'JKV', 'JKQ', \n",
    "            'JX', 'JC',\n",
    "            'SF', 'SP', 'SS', 'SE', 'SO',\n",
    "            'EP', 'EF', 'EC', 'ETN', 'ETM',\n",
    "            'XSN', 'XSV', 'XSA','VV'\n",
    "        ]\n",
    "        # 관계언(격조사,보조사,접속조사) \n",
    "        # 기호(마침표,물음표,느낌표,쉼포,가운뎃점,콜론,빗금,따옴표,괄호표,줄표,줄임표,붙임표)\n",
    "        # 의존형태(어미,접두사,접미사)\n",
    "        # 더 추가가능!!! 동사 제거해볼까 ????? \n",
    "        \n",
    "    # 형태소 분석기 POS 태거\n",
    "    def pos(self, sentence):\n",
    "        return self.komoran.pos(sentence)\n",
    "\n",
    "    # 불용어 제거 후, 필요한 품사 정보만 가져오기\n",
    "    def get_keywords(self, pos, without_tag=False):\n",
    "        f = lambda x: x in self.exclusion_tags\n",
    "        word_list = []\n",
    "        for p in pos:\n",
    "            if f(p[1]) is False:\n",
    "                word_list.append(p if without_tag is False else p[0])\n",
    "        return word_list\n",
    "    \n",
    "    # 키워드를 단어 인덱스 스퀀스로 변환\n",
    "    def get_wordidx_sequence(self, keywords):\n",
    "        if self.word_index is None:\n",
    "            return []\n",
    "        w2i = []\n",
    "        for word in keywords:\n",
    "            try:\n",
    "                w2i.append(self.word_index[word])\n",
    "            except KeyError:\n",
    "                #해당 단어가 사전에 없는 경우 OOV 처리\n",
    "                w2i.append(self.word_index['OOV'])\n",
    "        return w2i\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 챗봇 전처리 클래스 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent = '3년제 전문학사 졸업하고 학점은행제를 통해서 학사학위를 받을 예정인데 학력기준을 충족하나요?'\n",
    "# p = Preprocess(userdic='user_dic.tsv')\n",
    "\n",
    "# pos = p.pos(sent)\n",
    "\n",
    "# ret = p.get_keywords(pos, without_tag=False)\n",
    "# print(ret)\n",
    "\n",
    "# ret = p.get_keywords(pos, without_tag=True)\n",
    "# print(ret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 말뭉치 데이터 읽어오기\n",
    "# def read_corpus_data(filename):\n",
    "#     with open(filename, 'r', encoding=\"utf-8\") as f:\n",
    "#         data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "#     return data\n",
    "\n",
    "# corpus_data = read_corpus_data('raw_corpus.txt')\n",
    "\n",
    "# p = Preprocess()\n",
    "# dict = []\n",
    "\n",
    "# for c in corpus_data:\n",
    "#     pos = p.pos(c[0])\n",
    "#     keywords = p.get_keywords(pos, without_tag=True)\n",
    "#     for k in keywords:\n",
    "#         dict.append(k) # 키워드만 추출한 사전\n",
    "        \n",
    "# tokenizer = preprocessing.text.Tokenizer(oov_token='OOV')\n",
    "# tokenizer.fit_on_texts(dict)\n",
    "# word_index = tokenizer.word_index\n",
    "        \n",
    "# print(word_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단어 사전 생성 \n",
    "#### raw_chatbot_dict.bin   id, document, lable column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras import preprocessing\n",
    "# import pickle\n",
    "\n",
    "# # 말뭉치 데이터 읽어오기\n",
    "# def read_corpus_data(filename):\n",
    "#     with open(filename, 'r', encoding=\"utf-8\") as f:\n",
    "#         data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "#     return data\n",
    "\n",
    "\n",
    "# # 말뭉치 데이터 가져오기\n",
    "# corpus_data = read_corpus_data('raw_corpus.txt')\n",
    "\n",
    "\n",
    "# # 망뭉치 데이터에서 키워드만 추출해서 사전 리스트 생성\n",
    "# p = Preprocess()\n",
    "# dict = []\n",
    "# for c in corpus_data:\n",
    "#     pos = p.pos(c[0])\n",
    "#     keywords = p.get_keywords(pos, without_tag=True)\n",
    "#     for k in keywords:\n",
    "#         dict.append(k) # 키워드만 추출한 사전\n",
    "\n",
    "# # 사전에 사용될 word2index 생성\n",
    "# # 사전의 첫번 째 인덱스에는 OOV 사용\n",
    "# tokenizer = preprocessing.text.Tokenizer(oov_token='OOV')\n",
    "# tokenizer.fit_on_texts(dict)\n",
    "# word_index = tokenizer.word_index\n",
    "\n",
    "# # 사전 파일 생성\n",
    "# f = open(\"raw_chatbot_dict.bin\", \"wb\")\n",
    "# try:\n",
    "#     pickle.dump(word_index, f)\n",
    "# except Exception as e:\n",
    "#     print(e)\n",
    "# finally:\n",
    "#     f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단어 사전 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # 단어 사전 불러오기\n",
    "# f = open(\"raw_chatbot_dict.bin\", \"rb\")\n",
    "# word_index = pickle.load(f)\n",
    "# f.close()\n",
    "\n",
    "# sent = \"MOOC 기초과정 학습을 따로 진행할 수 있는 방법이 있을지 궁금합니다.\"\n",
    "\n",
    "# # 전처리 객체 생성\n",
    "# p = Preprocess(userdic='user_dic.tsv')\n",
    "\n",
    "# # 형태소분석기 실행\n",
    "# pos = p.pos(sent)\n",
    "\n",
    "# # 품사 태그 없이 키워드 출력\n",
    "# keywords = p.get_keywords(pos, without_tag=True)\n",
    "# for word in keywords:\n",
    "#     try:\n",
    "#         print(word, word_index[word])\n",
    "#     except KeyError:\n",
    "#         # 해당 단어가 사전에 없는 경우, OOV 처리\n",
    "#         print(word, word_index['OOV'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 글로벌 파라미터 정보( 챗봇 엔진 소스 전역에서 사용할 파라미터 정보)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 시퀀스 벡터 크기\n",
    "MAX_SEQ_LEN = 15\n",
    "\n",
    "def GlobalParams():\n",
    "    global MAX_SEQ_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 필요한 모듈 임포트\n",
    "# import pandas as pd\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import preprocessing\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate\n",
    "\n",
    "\n",
    "# # 데이터 읽어오기\n",
    "# train_file = \"raw_total_train_data.csv\" # 우리데이터로 수정 !!! \n",
    "# data = pd.read_csv(train_file, delimiter=',')\n",
    "# queries = data['query'].tolist()\n",
    "# intents = data['intent'].tolist()\n",
    "\n",
    "# p = Preprocess(word2index_dic='raw_chatbot_dict.bin', userdic='user_dic.tsv') # 우리 데이터로 수정 !!!\n",
    "\n",
    "# # 단어 시퀀스 생성\n",
    "# sequences = []\n",
    "# for sentence in queries:\n",
    "#     pos = p.pos(sentence)\n",
    "#     keywords = p.get_keywords(pos, without_tag=True)\n",
    "#     seq = p.get_wordidx_sequence(keywords)\n",
    "#     sequences.append(seq)\n",
    "    \n",
    "# padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
    "\n",
    "# # (105658, 15)\n",
    "# print(padded_seqs.shape)\n",
    "# print(len(intents)) #105658"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 의도 분류 모델 학습\n",
    "#### chat_intent_model.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 필요한 모듈 임포트\n",
    "# import pandas as pd\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import preprocessing\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate\n",
    "\n",
    "\n",
    "# # 데이터 읽어오기\n",
    "# train_file = \"raw_total_train_data.csv\" # 우리데이터로 수정 !!! \n",
    "# # train_file = \"models/intent/total_train_data.csv\"\n",
    "# data = pd.read_csv(train_file, delimiter=',')\n",
    "# queries = data['query'].tolist()\n",
    "# intents = data['intent'].tolist()\n",
    "\n",
    "# p = Preprocess(word2index_dic='raw_chatbot_dict.bin', userdic='user_dic.tsv') # 우리 데이터로 수정 !!!\n",
    "\n",
    "# # 단어 시퀀스 생성\n",
    "# sequences = []\n",
    "# for sentence in queries:\n",
    "#     pos = p.pos(sentence)\n",
    "#     keywords = p.get_keywords(pos, without_tag=True)\n",
    "#     seq = p.get_wordidx_sequence(keywords)\n",
    "#     sequences.append(seq)\n",
    "\n",
    "\n",
    "# # 단어 인덱스 시퀀스 벡터 ○2\n",
    "# # 단어 시퀀스 벡터 크기\n",
    "# # from config.GlobalParams import MAX_SEQ_LEN\n",
    "# padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
    "\n",
    "# # (105658, 15)\n",
    "# print(padded_seqs.shape)\n",
    "# print(len(intents)) #105658\n",
    "\n",
    "# # 학습용, 검증용, 테스트용 데이터셋 생성 ○3\n",
    "# # 학습셋:검증셋:테스트셋 = 7:2:1\n",
    "# ds = tf.data.Dataset.from_tensor_slices((padded_seqs, intents))\n",
    "# ds = ds.shuffle(len(queries))\n",
    "\n",
    "# train_size = int(len(padded_seqs) * 0.7)\n",
    "# val_size = int(len(padded_seqs) * 0.2)\n",
    "# test_size = int(len(padded_seqs) * 0.1)\n",
    "\n",
    "# train_ds = ds.take(train_size).batch(20)\n",
    "# val_ds = ds.skip(train_size).take(val_size).batch(20)\n",
    "# test_ds = ds.skip(train_size + val_size).take(test_size).batch(20)\n",
    "\n",
    "# # 하이퍼 파라미터 설정\n",
    "# dropout_prob = 0\n",
    "# EMB_SIZE = 128\n",
    "# EPOCH = 10\n",
    "# VOCAB_SIZE = len(p.word_index) + 1 #전체 단어 개수\n",
    "\n",
    "\n",
    "# # CNN 모델 정의  ○4\n",
    "# input_layer = Input(shape=(MAX_SEQ_LEN,))\n",
    "# embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length=MAX_SEQ_LEN)(input_layer)\n",
    "# dropout_emb = Dropout(rate=dropout_prob)(embedding_layer)\n",
    "\n",
    "# conv1 = Conv1D(\n",
    "#     filters=128,\n",
    "#     kernel_size=3,\n",
    "#     padding='valid',\n",
    "#     activation=tf.nn.relu)(dropout_emb)\n",
    "# pool1 = GlobalMaxPool1D()(conv1)\n",
    "\n",
    "# conv2 = Conv1D(\n",
    "#     filters=128,\n",
    "#     kernel_size=4,\n",
    "#     padding='valid',\n",
    "#     activation=tf.nn.relu)(dropout_emb)\n",
    "# pool2 = GlobalMaxPool1D()(conv2)\n",
    "\n",
    "# conv3 = Conv1D(\n",
    "#     filters=128,\n",
    "#     kernel_size=5,\n",
    "#     padding='valid',\n",
    "#     activation=tf.nn.relu)(dropout_emb)\n",
    "# pool3 = GlobalMaxPool1D()(conv3)\n",
    "\n",
    "# # 3,4,5gram 이후 합치기\n",
    "# concat = concatenate([pool1, pool2, pool3])\n",
    "\n",
    "# hidden = Dense(128, activation=tf.nn.relu)(concat)\n",
    "# dropout_hidden = Dropout(rate=dropout_prob)(hidden)\n",
    "# logits = Dense(5, name='logits')(dropout_hidden)\n",
    "# predictions = Dense(5, activation=tf.nn.softmax)(logits)\n",
    "\n",
    "\n",
    "# # 모델 생성  ○5\n",
    "# model = Model(inputs=input_layer, outputs=predictions)\n",
    "# model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# # 모델 학습 ○6\n",
    "# model.fit(train_ds, validation_data=val_ds, epochs=EPOCH, verbose=1)\n",
    "\n",
    "\n",
    "# # 모델 평가(테스트 데이터 셋 이용) ○7\n",
    "# loss, accuracy = model.evaluate(test_ds, verbose=1)\n",
    "# print('Accuracy: %f' % (accuracy * 100))\n",
    "# print('loss: %f' % (loss))\n",
    "\n",
    "\n",
    "# # 모델 저장  ○8\n",
    "# model.save('real_chat.h5') # 우리 데이터 모델로 이름 변경 !!! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 의도 분류 모듈 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "\n",
    "# 의도 분류 모델 모듈\n",
    "class IntentModel:\n",
    "    def __init__(self, model_name, proprocess):\n",
    "\n",
    "        # 의도 클래스 별 레이블\n",
    "        self.labels = {0 : \"인사\", 1 : \"사전학습안내\", 2 : \"서류제출안내\", 3 : \"운영일정안내\", 4 : \"지원자격안내\" }\n",
    "        # 의도 분류 모델 불러오기\n",
    "        self.model = load_model(model_name)\n",
    "\n",
    "        # 챗봇 Preprocess 객체\n",
    "        self.p = proprocess\n",
    "\n",
    "\n",
    "    # 의도 클래스 예측\n",
    "    def predict_class(self, query):\n",
    "        # 형태소 분석\n",
    "        pos = self.p.pos(query)\n",
    "\n",
    "        # 문장내 키워드 추출(불용어 제거)\n",
    "        keywords = self.p.get_keywords(pos, without_tag=True)\n",
    "        sequences = [self.p.get_wordidx_sequence(keywords)]\n",
    "\n",
    "        # 단어 시퀀스 벡터 크기\n",
    "        # from config.GlobalParams import MAX_SEQ_LEN\n",
    "\n",
    "        # 패딩처리\n",
    "        padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
    "\n",
    "        predict = self.model.predict(padded_seqs)\n",
    "        predict_class = tf.math.argmax(predict, axis=1)\n",
    "        return predict_class.numpy()[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IntentModel 클래스 태스트 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from models.intent.IntentModel import IntentModel\n",
    "\n",
    "# p = Preprocess(word2index_dic='chatbot_dict.bin', userdic='user_dic.tsv')\n",
    "\n",
    "# intent = IntentModel(model_name='real_chat.h5', proprocess=p)\n",
    "# query = \"필기시험 비대면으로 치나요?\"\n",
    "# predict = intent.predict_class(query)\n",
    "# predict_label = intent.labels[predict]\n",
    "\n",
    "# print(\"질문 :\",query)\n",
    "# print(\"의도 예측 클래스 : \", predict)\n",
    "# print(\"의도 예측 레이블 : \", predict_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 개체명 인식 모델 학습\n",
    "#### ner_model.h5 -> change "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import preprocessing\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import numpy as np\n",
    "\n",
    "# # 학습 파일 불러오기\n",
    "# def read_file(file_name):\n",
    "#     sents = []\n",
    "#     with open(file_name, 'r', encoding='utf-8') as f:\n",
    "#         lines = f.readlines()\n",
    "#         for idx, l in enumerate(lines):\n",
    "#             if l[0] == ';' and lines[idx + 1][0] == '$':\n",
    "#                 this_sent = []\n",
    "#             elif l[0] == '$' and lines[idx - 1][0] == ';':\n",
    "#                 continue\n",
    "#             elif l[0] == '\\n':\n",
    "#                 sents.append(this_sent)\n",
    "#             else:\n",
    "#                 this_sent.append(tuple(l.split()))\n",
    "#     return sents\n",
    "\n",
    "# p = Preprocess(word2index_dic='raw_chatbot_dict.bin', userdic='user_dic.tsv') # 우리 데이터로 수정 !!!\n",
    "\n",
    "# # 학습용 말뭉치 데이터를 불러옴\n",
    "# corpus = read_file('models/ner/ner_train.txt') # 우리 데이터로 수정 !!!\n",
    "\n",
    "# # 말뭉치 데이터에서 단어와 BIO 태그만 불러와 학습용 데이터셋 생성\n",
    "# sentences, tags = [], []\n",
    "# for t in corpus:\n",
    "#     tagged_sentence = []\n",
    "#     sentence, bio_tag = [], []\n",
    "#     for w in t:\n",
    "#         tagged_sentence.append((w[1], w[3]))\n",
    "#         sentence.append(w[1])\n",
    "#         bio_tag.append(w[3])\n",
    "    \n",
    "#     sentences.append(sentence)\n",
    "#     tags.append(bio_tag)\n",
    "\n",
    "\n",
    "# print(\"샘플 크기 : \\n\", len(sentences))\n",
    "# print(\"0번 째 샘플 단어 시퀀스 : \\n\", sentences[0])\n",
    "# print(\"0번 째 샘플 bio 태그 : \\n\", tags[0])\n",
    "# print(\"샘플 단어 시퀀스 최대 길이 :\", max(len(l) for l in sentences))\n",
    "# print(\"샘플 단어 시퀀스 평균 길이 :\", (sum(map(len, sentences))/len(sentences)))\n",
    "\n",
    "# # 토크나이저 정의\n",
    "# tag_tokenizer = preprocessing.text.Tokenizer(lower=False) # 태그 정보는 lower=False 소문자로 변환하지 않는다.\n",
    "# tag_tokenizer.fit_on_texts(tags)\n",
    "\n",
    "# # 단어사전 및 태그 사전 크기\n",
    "# vocab_size = len(p.word_index) + 1\n",
    "# tag_size = len(tag_tokenizer.word_index) + 1\n",
    "# print(\"BIO 태그 사전 크기 :\", tag_size)\n",
    "# print(\"단어 사전 크기 :\", vocab_size)\n",
    "\n",
    "# # 학습용 단어 시퀀스 생성\n",
    "# x_train = [p.get_wordidx_sequence(sent) for sent in sentences]\n",
    "# y_train = tag_tokenizer.texts_to_sequences(tags)\n",
    "\n",
    "# index_to_ner = tag_tokenizer.index_word # 시퀀스 인덱스를 NER로 변환 하기 위해 사용\n",
    "# index_to_ner[0] = 'PAD'\n",
    "\n",
    "# # 시퀀스 패딩 처리\n",
    "# max_len = 40\n",
    "# x_train = preprocessing.sequence.pad_sequences(x_train, padding='post', maxlen=max_len)\n",
    "# y_train = preprocessing.sequence.pad_sequences(y_train, padding='post', maxlen=max_len)\n",
    "\n",
    "# # 학습 데이터와 테스트 데이터를 8:2의 비율로 분리\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x_train, y_train,\n",
    "#                                                     test_size=.2,\n",
    "#                                                     random_state=1234)\n",
    "\n",
    "# # 출력 데이터를 one-hot encoding\n",
    "# y_train = tf.keras.utils.to_categorical(y_train, num_classes=tag_size)\n",
    "# y_test = tf.keras.utils.to_categorical(y_test, num_classes=tag_size)\n",
    "\n",
    "# print(\"학습 샘플 시퀀스 형상 : \", x_train.shape)\n",
    "# print(\"학습 샘플 레이블 형상 : \", y_train.shape)\n",
    "# print(\"테스트 샘플 시퀀스 형상 : \", x_test.shape)\n",
    "# print(\"테스트 샘플 레이블 형상 : \", y_test.shape)\n",
    "\n",
    "\n",
    "# # 모델 정의 (Bi-LSTM)\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=vocab_size, output_dim=30, input_length=max_len, mask_zero=True))\n",
    "# model.add(Bidirectional(LSTM(200, return_sequences=True, dropout=0.50, recurrent_dropout=0.25)))\n",
    "# model.add(TimeDistributed(Dense(tag_size, activation='softmax')))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer=Adam(0.01), metrics=['accuracy'])\n",
    "# model.fit(x_train, y_train, batch_size=128, epochs=10)\n",
    "\n",
    "# print(\"평가 결과 : \", model.evaluate(x_test, y_test)[1])\n",
    "# model.save('ner_model.h5') # chat_ner로 수정 !!!\n",
    "\n",
    "\n",
    "# # 시퀀스를 NER 태그로 변환\n",
    "# def sequences_to_tag(sequences):  # 예측값을 index_to_ner를 사용하여 태깅 정보로 변경하는 함수.\n",
    "#     result = []\n",
    "#     for sequence in sequences:  # 전체 시퀀스로부터 시퀀스를 하나씩 꺼낸다.\n",
    "#         temp = []\n",
    "#         for pred in sequence:  # 시퀀스로부터 예측값을 하나씩 꺼낸다.\n",
    "#             pred_index = np.argmax(pred)  # 예를 들어 [0, 0, 1, 0 ,0]라면 1의 인덱스인 2를 리턴한다.\n",
    "#             temp.append(index_to_ner[pred_index].replace(\"PAD\", \"O\"))  # 'PAD'는 'O'로 변경\n",
    "#         result.append(temp)\n",
    "#     return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1-score CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # f1 스코어 계산을 위해 사용\n",
    "# from seqeval.metrics import f1_score, classification_report\n",
    "\n",
    "\n",
    "# # 테스트 데이터셋의 NER 예측\n",
    "# y_predicted = model.predict(x_test)\n",
    "# pred_tags = sequences_to_tag(y_predicted) # 예측된 NER\n",
    "# test_tags = sequences_to_tag(y_test)    # 실제 NER\n",
    "\n",
    "# # F1 평가 결과\n",
    "# print(classification_report(test_tags, pred_tags))\n",
    "# print(\"F1-score: {:.1%}\".format(f1_score(test_tags, pred_tags)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 개체명 인식 모듈 생성 --> 우리껄로 바꿔야댐 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "\n",
    "# 개체명 인식 모델 모듈\n",
    "class NerModel:\n",
    "    def __init__(self, model_name, proprocess):\n",
    "\n",
    "        # BIO 태그 클래스 별 레이블\n",
    "        self.index_to_ner = {1: 'O', 2: 'B_DT', 3: 'B_FOOD', 4: 'I', 5: 'B_OG', 6: 'B_PS', 7: 'B_LC', 8: 'NNP', 9: 'B_TI', 0: 'PAD'}\n",
    "\n",
    "        # 의도 분류 모델 불러오기\n",
    "        self.model = load_model(model_name)\n",
    "\n",
    "        # 챗봇 Preprocess 객체\n",
    "        self.p = proprocess\n",
    "\n",
    "\n",
    "    # 개체명 클래스 예측\n",
    "    def predict(self, query):\n",
    "        # 형태소 분석\n",
    "        pos = self.p.pos(query)\n",
    "\n",
    "        # 문장내 키워드 추출(불용어 제거)\n",
    "        keywords = self.p.get_keywords(pos, without_tag=True)\n",
    "        sequences = [self.p.get_wordidx_sequence(keywords)]\n",
    "\n",
    "        # 패딩처리\n",
    "        max_len = 40\n",
    "        padded_seqs = preprocessing.sequence.pad_sequences(sequences, padding=\"post\", value=0, maxlen=max_len)\n",
    "\n",
    "        predict = self.model.predict(np.array([padded_seqs[0]]))\n",
    "        predict_class = tf.math.argmax(predict, axis=-1)\n",
    "\n",
    "        tags = [self.index_to_ner[i] for i in predict_class.numpy()[0]]\n",
    "        return list(zip(keywords, tags))\n",
    "\n",
    "    def predict_tags(self, query):\n",
    "        # 형태소 분석\n",
    "        pos = self.p.pos(query)\n",
    "\n",
    "        # 문장내 키워드 추출(불용어 제거)\n",
    "        keywords = self.p.get_keywords(pos, without_tag=True)\n",
    "        sequences = [self.p.get_wordidx_sequence(keywords)]\n",
    "\n",
    "        # 패딩처리\n",
    "        max_len = 40\n",
    "        padded_seqs = preprocessing.sequence.pad_sequences(sequences, padding=\"post\", value=0, maxlen=max_len)\n",
    "\n",
    "        predict = self.model.predict(np.array([padded_seqs[0]]))\n",
    "        predict_class = tf.math.argmax(predict, axis=-1)\n",
    "\n",
    "        tags = []\n",
    "        for tag_idx in predict_class.numpy()[0]:\n",
    "            if tag_idx == 1: continue\n",
    "            tags.append(self.index_to_ner[tag_idx])\n",
    "\n",
    "        if len(tags) == 0: return None\n",
    "        return tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NerModel 객체 사용 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.ner.NerModel import NerModel\n",
    "\n",
    "# p = Preprocess(word2index_dic='chatbot_dict.bin', userdic='utils/user_dic.tsv')\n",
    "\n",
    "\n",
    "# ner = NerModel(model_name='ner_model.h5', proprocess=p) # chat_ner로 수정 !!!\n",
    "# query = '안녕하세요 제 이름은 포림이에요'\n",
    "# predicts = ner.predict(query)\n",
    "# tags = ner.predict_tags(query)\n",
    "# print(predicts)\n",
    "# print(tags)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 여기서부터 서버 연동하는거 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터베이스 제어 모듈 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import pymysql.cursors\n",
    "import logging\n",
    "\n",
    "\n",
    "class Database:\n",
    "    '''\n",
    "    database 제어\n",
    "    '''\n",
    "\n",
    "    def __init__(self, host, user, password, db_name, charset='utf8'):\n",
    "        self.host = host\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.charset = charset\n",
    "        self.db_name = db_name\n",
    "        self.conn = None\n",
    "\n",
    "    # DB 연결\n",
    "    def connect(self):\n",
    "        if self.conn != None:\n",
    "            return\n",
    "\n",
    "        self.conn = pymysql.connect(\n",
    "            host=self.host,\n",
    "            user=self.user,\n",
    "            password=self.password,\n",
    "            db=self.db_name,\n",
    "            charset=self.charset\n",
    "        )\n",
    "\n",
    "    # DB 연결 닫기\n",
    "    def close(self):\n",
    "        if self.conn is None:\n",
    "            return\n",
    "\n",
    "        if not self.conn.open:\n",
    "            self.conn = None\n",
    "            return\n",
    "        self.conn.close()\n",
    "        self.conn = None\n",
    "\n",
    "    # SQL 구문 실행\n",
    "    def execute(self, sql):\n",
    "        last_row_id = -1\n",
    "        try:\n",
    "            with self.conn.cursor() as cursor:\n",
    "                cursor.execute(sql)\n",
    "            self.conn.commit()\n",
    "            last_row_id = cursor.lastrowid\n",
    "            # logging.debug(\"excute last_row_id : %d\", last_row_id)\n",
    "        except Exception as ex:\n",
    "            logging.error(ex)\n",
    "\n",
    "        finally:\n",
    "            return last_row_id\n",
    "\n",
    "    # SELECT 구문 실행 후, 단 1개의 데이터 ROW만 불러옴\n",
    "    def select_one(self, sql):\n",
    "        result = None\n",
    "\n",
    "        try:\n",
    "            with self.conn.cursor(pymysql.cursors.DictCursor) as cursor:\n",
    "                cursor.execute(sql)\n",
    "                result = cursor.fetchone()\n",
    "        except Exception as ex:\n",
    "            logging.error(ex)\n",
    "\n",
    "        finally:\n",
    "            return result\n",
    "\n",
    "    # SELECT 구문 실행 후, 전체 데이터 ROW만 불러옴\n",
    "    def select_all(self, sql):\n",
    "        result = None\n",
    "\n",
    "        try:\n",
    "            with self.conn.cursor(pymysql.cursors.DictCursor) as cursor:\n",
    "                cursor.execute(sql)\n",
    "                result = cursor.fetchall()\n",
    "        except Exception as ex:\n",
    "            logging.error(ex)\n",
    "\n",
    "        finally:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 챗봇 답변 검색 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FindAnswer:\n",
    "    def __init__(self, db):\n",
    "        self.db = db\n",
    "\n",
    "    # 검색 쿼리 생성\n",
    "    def _make_query(self, intent_name, ner_tags):\n",
    "        sql = \"select * from chatbot\" ## 바꿔야 할듯? \n",
    "        if intent_name != None and ner_tags == None:\n",
    "            sql = sql + \" where intent='{}' \".format(intent_name)\n",
    "\n",
    "        elif intent_name != None and ner_tags != None:\n",
    "            where = ' where intent=\"%s\" ' % intent_name\n",
    "            if (len(ner_tags) > 0):\n",
    "                where += 'and ('\n",
    "                for ne in ner_tags:\n",
    "                    where += \" ner like '%{}%' or \".format(ne)\n",
    "                where = where[:-3] + ')'\n",
    "            sql = sql + where\n",
    "\n",
    "        # 동일한 답변이 2개 이상인 경우, 랜덤으로 선택\n",
    "        sql = sql + \" order by rand() limit 1\"\n",
    "        return sql\n",
    "\n",
    "    # 답변 검색\n",
    "    def search(self, intent_name, ner_tags):\n",
    "        # 의도명, 개체명으로 답변 검색\n",
    "        sql = self._make_query(intent_name, ner_tags)\n",
    "        answer = self.db.select_one(sql)\n",
    "\n",
    "        # 검색되는 답변이 없으면 의도명만 검색\n",
    "        if answer is None:\n",
    "            sql = self._make_query(intent_name, None)\n",
    "            answer = self.db.select_one(sql)\n",
    "\n",
    "        return (answer['answer'], answer['answer_image'])\n",
    "\n",
    "    # NER 태그를 실제 입력된 단어로 변환\n",
    "    def tag_to_word(self, ner_predicts, answer):\n",
    "        for word, tag in ner_predicts:\n",
    "\n",
    "            # 변환해야하는 태그가 있는 경우 추가\n",
    "            if tag == 'B_FOOD' or tag == 'B_DT' or tag == 'B_TI':\n",
    "                answer = answer.replace(tag, word)\n",
    "\n",
    "        answer = answer.replace('{', '')\n",
    "        answer = answer.replace('}', '')\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 챗봇 엔진 동작 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.DatabaseConfig import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 전처리 객체 생성\n",
    "p = Preprocess(word2index_dic='raw_chatbot_dict.bin', userdic='user_dic.tsv')\n",
    "\n",
    "# 질문/답변 학습 디비 연결 객체 생성\n",
    "db = Database(\n",
    "    host='localhost', user='root', password='aiproject', db_name='chatbot'\n",
    ")\n",
    "db.connect()    # 디비 연결\n",
    "\n",
    "# 원문\n",
    "query = \"사전학습을 필수적으로 들어야하나요?\"\n",
    "# query = \"화자의 질문 의도를 파악합니다.\"\n",
    "# query = \"안녕하세요\"\n",
    "# query = \"자장면 주문할게요\"\n",
    "\n",
    "# 의도 파악\n",
    "# from models.intent.IntentModel import IntentModel\n",
    "intent = IntentModel(model_name='chat_intent_model.h5', proprocess=p)\n",
    "predict = intent.predict_class(query)\n",
    "intent_name = intent.labels[predict]\n",
    "\n",
    "# 개체명 인식\n",
    "# from models.ner.NerModel import NerModel\n",
    "ner = NerModel(model_name='ner_model.h5', proprocess=p)\n",
    "predicts = ner.predict(query)\n",
    "ner_tags = ner.predict_tags(query)\n",
    "\n",
    "print(\"질문 : \", query)\n",
    "print(\"=\" * 40)\n",
    "print(\"의도 파악 : \", intent_name)\n",
    "print(\"개체명 인식 : \", predicts)\n",
    "print(\"답변 검색에 필요한 NER 태그 : \", ner_tags)\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 답변 검색\n",
    "# from utils.FindAnswer import FindAnswer\n",
    "\n",
    "try:\n",
    "    f = FindAnswer(db)\n",
    "    answer_text, answer_image = f.search(intent_name, ner_tags)\n",
    "    answer = f.tag_to_word(predicts, answer_text)\n",
    "except:\n",
    "    answer = \"죄송해요 무슨 말인지 모르겠어요\"\n",
    "\n",
    "print(\"답변 : \", answer)\n",
    "\n",
    "db.close() # 디비 연결 끊음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error !!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 챗봇 서버 모듈 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "\n",
    "class BotServer:\n",
    "    def __init__(self, srv_port, listen_num):\n",
    "        self.port = srv_port\n",
    "        self.listen = listen_num\n",
    "        self.mySock = None\n",
    "\n",
    "    # sock 생성\n",
    "    def create_sock(self):\n",
    "        self.mySock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        self.mySock.bind((\"0.0.0.0\", int(self.port)))\n",
    "        self.mySock.listen(int(self.listen))\n",
    "        return self.mySock\n",
    "\n",
    "    # client 대기\n",
    "    def ready_for_client(self):\n",
    "        return self.mySock.accept()\n",
    "\n",
    "    # sock 반환\n",
    "    def get_sock(self):\n",
    "        return self.mySock\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 챗봇 엔진 서버 메인 프로그램 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "DB 접속\n",
      "bot start\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import json\n",
    "\n",
    "from models.intent.IntentModel import IntentModel\n",
    "from models.ner.NerModel import NerModel\n",
    "\n",
    "\n",
    "# 전처리 객체 생성\n",
    "p = Preprocess(word2index_dic='raw_chatbot_dict.bin', userdic='user_dic.tsv')\n",
    "\n",
    "# 의도 파악 모델\n",
    "intent = IntentModel(model_name='real_chat.h5', proprocess=p)\n",
    "\n",
    "# 개체명 인식 모델\n",
    "ner = NerModel(model_name='ner_model.h5', proprocess=p) # chat_ner로 수정 !!!\n",
    "\n",
    "\n",
    "def to_client(conn, addr, params):\n",
    "    db = params['db']\n",
    "\n",
    "    try:\n",
    "        db.connect()  # 디비 연결\n",
    "\n",
    "        # 데이터 수신\n",
    "        read = conn.recv(2048)  # 수신 데이터가 있을 때 까지 블로킹\n",
    "        print('===========================')\n",
    "        print('Connection from: %s' % str(addr))\n",
    "\n",
    "        if read is None or not read:\n",
    "            # 클라이언트 연결이 끊어지거나, 오류가 있는 경우\n",
    "            print('클라이언트 연결 끊어짐')\n",
    "            exit(0)\n",
    "\n",
    "\n",
    "        # json 데이터로 변환\n",
    "        recv_json_data = json.loads(read.decode())\n",
    "        print(\"데이터 수신 : \", recv_json_data)\n",
    "        query = recv_json_data['Query']\n",
    "\n",
    "        # 의도 파악\n",
    "        intent_predict = intent.predict_class(query)\n",
    "        intent_name = intent.labels[intent_predict]\n",
    "\n",
    "        # 개체명 파악\n",
    "        ner_predicts = ner.predict(query)\n",
    "        ner_tags = ner.predict_tags(query)\n",
    "\n",
    "\n",
    "        # 답변 검색\n",
    "        try:\n",
    "            f = FindAnswer(db)\n",
    "            answer_text, answer_image = f.search(intent_name, ner_tags)\n",
    "            answer = f.tag_to_word(ner_predicts, answer_text)\n",
    "\n",
    "        except:\n",
    "            answer = \"죄송해요 무슨 말인지 모르겠어요. 조금 더 공부 할게요.\"\n",
    "            answer_image = None\n",
    "\n",
    "        send_json_data_str = {\n",
    "            \"Query\" : query,\n",
    "            \"Answer\": answer,\n",
    "            \"AnswerImageUrl\" : answer_image,\n",
    "            \"Intent\": intent_name,\n",
    "            \"NER\": str(ner_predicts)\n",
    "        }\n",
    "        message = json.dumps(send_json_data_str)\n",
    "        conn.send(message.encode())\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "\n",
    "    finally:\n",
    "        if db is not None: # db 연결 끊기\n",
    "            db.close()\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # 질문/답변 학습 디비 연결 객체 생성\n",
    "    db = Database(\n",
    "        host='localhost', user='root', password='aiproject', db_name='chatbot'\n",
    "    )\n",
    "    print(\"DB 접속\")\n",
    "\n",
    "    port = 5050\n",
    "    listen = 100\n",
    "\n",
    "    # 봇 서버 동작\n",
    "    bot = BotServer(port, listen)\n",
    "    bot.create_sock()\n",
    "    print(\"bot start\")\n",
    "\n",
    "    while True:\n",
    "        conn, addr = bot.ready_for_client()\n",
    "        params = {\n",
    "            \"db\": db\n",
    "        }\n",
    "\n",
    "        client = threading.Thread(target=to_client, args=(\n",
    "            conn,\n",
    "            addr,\n",
    "            params\n",
    "        ))\n",
    "        client.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5d240ba0dc525c389faa33f5dcce5b4f32b6d6aa6d70d6d2dd929bd2b09ab69f"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
